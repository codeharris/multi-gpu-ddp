experiment_name: "imdb_single_gpu_transformer"
output_dir: "experiments/exp010_imdb_single_gpu"

seed: 42

model:
  type: "transformer_sequence_classifier"
  d_model: 128
  n_heads: 4
  num_layers: 2
  dim_feedforward: 256
  dropout: 0.1
  vocab_size: 20000      # hash-based vocab size
  num_classes: 2
  max_seq_len: 128       # typical for IMDB reviews

data:
  dataset: "imdb_hash"

training:
  epochs: 3
  batch_size: 32

distributed:
  use_ddp: false         # IMPORTANT: start with single GPU
  backend: "nccl"
