# AG News Classification with DistilBERT (Single GPU Baseline)
# DistilBERT: ~66M parameters, should finally show positive speedup with DDP

experiment_name: "ag_news_distilbert_1gpu"
output_dir: "experiments/exp045_agnews_distilbert_1gpu"

seed: 42

model:
  type: "distilbert"  # Use DistilBERT instead of custom transformer
  pretrained: true    # Use pre-trained weights from HuggingFace
  num_classes: 4
  max_seq_len: 256    # DistilBERT can handle up to 512

data:
  dataset: "ag_news_distilbert"  # Uses real tokenizer
  num_workers: 4
  pin_memory: true

training:
  epochs: 10          # Reduced epochs since pre-trained
  batch_size: 32      # 32 per GPU (larger batch for bigger model)
  learning_rate: 2e-5 # Lower LR for fine-tuning pre-trained model

distributed:
  use_ddp: false
  backend: "nccl"
